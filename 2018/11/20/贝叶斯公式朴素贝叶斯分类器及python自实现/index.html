<!DOCTYPE html>
<html lang="">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="wenwen">



	

<title>贝叶斯公式/朴素贝叶斯分类器及python自实现 | wnewen&#39;s blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Wenwen&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Post</a>
                
                    <a class="menu-item" href="/category">Category</a>
                
                    <a class="menu-item" href="/tag">Tag</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Wenwen&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Post</a>
                
                    <a class="menu-item" href="/category">Category</a>
                
                    <a class="menu-item" href="/tag">Tag</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>

        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">贝叶斯公式/朴素贝叶斯分类器及python自实现</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">wenwen</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">November 20, 2018&nbsp;&nbsp;9:48:40</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Machine-Learning/">Machine Learning</a>
                            
                        </span>
                    
					<span ></span>
					<span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span> visits</span>
                </div>
            
        </header>

        <div class="post-content">
            <p>本文从贝叶斯与频率概率的对比入手理解贝叶斯决策的思维方式。通过两个实例理解贝叶斯的思想与流程，然后梳理了朴素贝叶斯分类器的算法流程，最后从零开始实现了朴素分类器的算法。</p>
<h3 id="1-起源、提出与贝叶斯公式"><a href="#1-起源、提出与贝叶斯公式" class="headerlink" title="1.起源、提出与贝叶斯公式"></a>1.起源、提出与贝叶斯公式</h3><p>贝叶斯派别主要是与古典统计学相比较区分的。</p>
<p>古典统计学基于大数定理，将一件事件经过大量重复实验得到的频率作为事件发生的概率，如常说的掷一枚硬币，正面朝上的概率为0.5。但，如果事先知道硬币的密度分布并不均匀，那此时的概率是不是就不为0.5呢？这种不同于“非黑即白”的思考方式，就是贝叶斯式的思考方式。</p>
<p>贝叶斯除了提出上述思考方式之外，还特别提出了举世闻名的贝叶斯定理。贝叶斯公式：</p>
<p>$$ P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_j)}$$</p>
<p>这里通过全概率公式，我们可以将上式等同于</p>
<p>$$ P(B|A) = \frac{P(B)P(A|B)}{P(A)} = P(B)\frac{P(A|B)}{P(A)}$$</p>
<p>右边的分式中，分子的P(A)称为先验概率，是B事件未发生前，对A事件概率的判断；P(A|B)即是在B事件发生之后，对A事件发生的后验概率。这整个分式我们也称之为‘’可能性函数(Likelyhood)‘’，这是一个调整因子，对A事件未发生之前B的概率进行调整，以得到A事件发生的前提下B事件发生的后验概率P(B|A)。</p>
<p>以一句话与一张图概括：贝叶斯定理是一种在已知其他概率的情况下求概率的方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20181120212448460.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzNTkwOTIx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="2-以实例感受贝叶斯决策：癌症病人计算-问题"><a href="#2-以实例感受贝叶斯决策：癌症病人计算-问题" class="headerlink" title="2.以实例感受贝叶斯决策：癌症病人计算 问题"></a>2.以实例感受贝叶斯决策：癌症病人计算 问题</h3><blockquote>
<p>有一家医院为了研究癌症的诊断，对一大批人作了一次普查，给每人打了试验针，然后进行统计，得到如下统计数字：</p>
<ol>
<li>这批人中，每1000人有5个癌症病人；</li>
<li>这批人中，每100个正常人有1人对试验的反应为阳性</li>
<li>这批人中，每100个癌症病人有95入对试验的反应为阳性。</li>
</ol>
<p>通过普查统计，该医院可开展癌症诊断。现在某人试验结果为阳性，诊断结果是什么?</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20181120212247159.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzNTkwOTIx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>(全是公式实在不好打，就用自己笔记代替啦)</p>
<p>上述例子是基于<strong>最小错误率贝叶斯决策</strong>，即哪类的概率大判断为哪类，也是我们常见的分类决策方法。但是将正常人错判为癌症病人，与将癌症病人错判为正常人的损失是不一样的。将正常人错判为癌症病人，会给他带来短期的精神负担，造成一定的损失，这个损失比较小。如果把癌症病人错判为正常人，致使患者失去挽救的机会，这个损失就大了。这两种不同的错判所造成损失的程度是有显著差别的。</p>
<p>所以，在决策时还要考虑到各种错判所造成的不同损失，由此提出了<strong>最小风险贝叶斯决策</strong>。</p>
<p>我们将$I_{ij}$记为将j类误判为i类所造成的损失。此处类别为2，如将样本x判为癌症病人$c_1$造成损失的数学期望为：</p>
<p>$R_1=I_{11}P(C_1|X)+I_{12}P(C_2|X)$</p>
<p>同理，将样本x判为癌症病人$c_2$造成损失的数学期望为</p>
<p>$R_2=I_{21}P(C_1|X)+I_{22}P(C_2|X)$</p>
<p>选择最小风险作为决策准则，若$R_1&lt;R_2$，则样本X$\epsilon R_1$，否则X$\epsilon R_2$</p>
<h3 id="3-以实例感受贝叶斯修正先验概率：狼来了"><a href="#3-以实例感受贝叶斯修正先验概率：狼来了" class="headerlink" title="3.以实例感受贝叶斯修正先验概率：狼来了"></a>3.以实例感受贝叶斯修正先验概率：狼来了</h3><blockquote>
<p>给出一些合理解释：</p>
<p>事件A表示:“小孩说谎”；事件B表示:“小孩可信”。</p>
<p>村民起初对这个小孩的信任度为0.8,即P(B)=0.8。</p>
<p>我们认为可信的孩子说谎的可能性为0.1。即P(A|B)=0.1。</p>
<p>不可信的孩子说谎的可能性为0.5,即P(A|^B)=0.5(用 ^B表示B的对立事件)。</p>
<p>求小孩第一次说谎、第二次说谎后，村民对这个小孩的信任度从P(B)=0.8会发生什么变化?</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20181120212414543.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIzNTkwOTIx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>由此我们可以得到贝叶斯决策另一个道理，<strong>贝叶斯决策会不断用后验概率，逐渐修正先验概率。</strong>投射到现实生活，我们也可以理解为当对某件事进行判断决策时，关于此事得到的信息越多，对此事的决策越准，而绝非yes/no的五五开决策。</p>
<blockquote>
<p>还是狼来的例子，如果这个孩子要改邪归正，他需要多少次才能把信任度提高到80%？</p>
</blockquote>
<p>如果要把信任度提高，那接下来就需要通过几次 不说谎 的经历来修正村民的观念，那可信度的计算，要记得将P(A|B)换成P((fei)A|B)，(A上方的横线打不出来…)，就是可信的孩子不说谎的可能性为1-P(A|B)=1-0.1，同样，不可信的孩子不说谎的概率为1-0.5</p>
<p>此时，我们就要用代码来实现而不是手算啦。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateTrustDegree</span><span class="params">(pb)</span>:</span></span><br><span class="line">  PB_A = float((<span class="number">1</span>-PA_B)*pb)/((<span class="number">1</span>-PA_B)*pb+(<span class="number">1</span>-PA_Bf)*(<span class="number">1</span>-pb))</span><br><span class="line">  <span class="keyword">return</span> PB_A</span><br><span class="line">PA_B = <span class="number">0.1</span></span><br><span class="line">PA_Bf = <span class="number">0.5</span></span><br><span class="line">pb = <span class="number">0.138</span></span><br><span class="line">N = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(pb &lt;=<span class="number">0.8</span>):</span><br><span class="line">    pb = calculateTrustDegree(pb)</span><br><span class="line">    N+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"He need &#123;0&#125; times with honest saying,villager can trust him again."</span>.format(N))</span><br><span class="line"></span><br><span class="line"><span class="comment"># N = 6</span></span><br><span class="line"><span class="comment"># He need 6 times with honest saying,villager can trust him again.</span></span><br></pre></td></tr></table></figure>
<h3 id="4-朴素贝叶斯分类器"><a href="#4-朴素贝叶斯分类器" class="headerlink" title="4.朴素贝叶斯分类器"></a>4.朴素贝叶斯分类器</h3><p><strong>贝叶斯分类算法</strong>是一大类分类算法的总称。</p>
<p>贝叶斯分类算法以<strong>样本可能属于某类的概率来作为分类依据</strong>。</p>
<p><strong>朴素贝叶斯分类算法</strong>是贝叶斯分类算法中最简单的一种，采用了<strong>“属性条件独立性假设”</strong>：对已知类别，假设所有属性互相独立，也就是在概率的计算时，可以将一个类别的每个属性直接相乘。这在概率论中应该学过，两个事件独立时，两个事件同时发生的概率等于两个事件分别发生的乘积。</p>
<p>给定一个属性值，其属于某个类的概率叫做条件概率。对于一个给定的类值，将每个属性的条件概率相乘，便得到一个数据样本属于某个类的概率。</p>
<p>我们可以通过计算样本归属于每个类的概率，然后选择具有最高概率的类来做预测。</p>
<p>我们以鸢尾花分类实例来过算法流程，并不适用sklearn库自实现朴素贝叶斯分类器。</p>
<blockquote>
<p>实例数据集介绍：鸢尾花数据集包含4维的特征，共三类150个样本，每类均有50个样本。</p>
</blockquote>
<p>算法流程概括如下：</p>
<ol>
<li>计算样本属于某类的<strong>先验概率</strong>，属于A类的概率为$\frac{属于A类的样本数}{所有样本数}$，以此类推</li>
<li>计算<strong>类条件概率</strong>，离散值通过类别数量比值，此数据集的属性特征为连续值所以通过 <strong>概率密度函数</strong> 来计算。首先计算在一个属性的前提下，该样本属于某类的概率；相乘合并所有属性的概率，即为某个数据样本属于某类的类条件概率 <ul>
<li>计算每个特征属于每类的条件概率<ul>
<li>概率密度函数实现</li>
<li>计算每个属性的均值和方差</li>
<li>按类别提取属性特征，这里会得到 类别数目*属性数目 组 （均值，方差）</li>
</ul>
</li>
<li>按类别将每个属性的条件概率相乘，如下所示<ul>
<li>判断为A类的概率：p(A|特征1)<em>p(A|特征2)</em>p(A|特征3)*p(A|特征4)…..</li>
<li>判断为B类的概率：p(B|特征1)<em>p(B|特征2)</em>p(B|特征3)*p(B|特征4)…..</li>
<li>判断为C类的概率：p(C|特征1)<em>p(C|特征2)</em>p(C|特征3)*p(C|特征4)…..</li>
</ul>
</li>
</ul>
</li>
<li><strong>先验概率*类条件概率</strong>，回顾一下贝叶斯公式，$$ P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_j)}$$。由于样本确定时，贝叶斯公式的分母都是相同的。所以判断样本属于哪类只需要比较分子部分：先验概率<em>类条件概率，最终属于哪类的概率最大，则判别为哪类，此处为*</em>最小错误率贝叶斯分类**，若采用最小风险需要加上判断为每个类别的风险损失值。</li>
</ol>
<h3 id="5-代码实现"><a href="#5-代码实现" class="headerlink" title="5.代码实现"></a>5.代码实现</h3><h4 id="1-数据集载入，划分训练集与测试集"><a href="#1-数据集载入，划分训练集与测试集" class="headerlink" title="1.数据集载入，划分训练集与测试集"></a>1.数据集载入，划分训练集与测试集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_df = pd.read_csv(<span class="string">'IrisData.csv'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitData</span><span class="params">(data_list,ratio)</span>:</span></span><br><span class="line">  train_size = int(len(data_list)*ratio)</span><br><span class="line">  random.shuffle(data_list)</span><br><span class="line">  train_set = data_list[:train_size]</span><br><span class="line">  test_set = data_list[train_size:]</span><br><span class="line">  <span class="keyword">return</span> train_set,test_set</span><br><span class="line"></span><br><span class="line">data_list = np.array(data_df).tolist()</span><br><span class="line">trainset,testset = splitData(data_list,ratio = <span class="number">0.7</span>)</span><br><span class="line">print(<span class="string">'Split &#123;0&#125; samples into &#123;1&#125; train and &#123;2&#125; test samples '</span>.format(len(data_df), len(trainset), len(testset)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split 150 samples into 105 train and 45 test samples</span></span><br></pre></td></tr></table></figure>
<h4 id="2-计算先验概率"><a href="#2-计算先验概率" class="headerlink" title="2.计算先验概率"></a>2.计算先验概率</h4><p>此时需要先知道数据集中属于各类别的样本分别有多少。我们通过一个函数实现按类别划分数据。</p>
<p>两个返回值分别为划分好的数据字典，以及划分好的数据集中每个类别的样本数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seprateByClass</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  seprate_dict = &#123;&#125;</span><br><span class="line">  info_dict = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> vector <span class="keyword">in</span> dataset:</span><br><span class="line">      <span class="keyword">if</span> vector[<span class="number">-1</span>] <span class="keyword">not</span> <span class="keyword">in</span> seprate_dict:</span><br><span class="line">          seprate_dict[vector[<span class="number">-1</span>]] = []</span><br><span class="line">          info_dict[vector[<span class="number">-1</span>]] = <span class="number">0</span></span><br><span class="line">      seprate_dict[vector[<span class="number">-1</span>]].append(vector)</span><br><span class="line">      info_dict[vector[<span class="number">-1</span>]] +=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> seprate_dict,info_dict</span><br><span class="line"></span><br><span class="line">train_separated,train_info = seprateByClass(trainset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_info：</span></span><br><span class="line"><span class="comment"># &#123;'Setosa': 41, 'Versicolour': 33, 'Virginica': 31&#125;</span></span><br></pre></td></tr></table></figure>
<p>计算属于每个类别的先验概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calulateClassPriorProb</span><span class="params">(dataset,dataset_info)</span>:</span></span><br><span class="line">  dataset_prior_prob = &#123;&#125;</span><br><span class="line">  sample_sum = len(dataset)</span><br><span class="line">  <span class="keyword">for</span> class_value, sample_nums <span class="keyword">in</span> dataset_info.items():</span><br><span class="line">      dataset_prior_prob[class_value] = sample_nums/float(sample_sum)</span><br><span class="line">  <span class="keyword">return</span> dataset_prior_prob</span><br><span class="line"></span><br><span class="line">prior_prob = calulateClassPriorProb(trainset,train_info)</span><br><span class="line"></span><br><span class="line"><span class="comment">#&#123;'Setosa': 0.3904761904761905,</span></span><br><span class="line"><span class="comment"># 'Versicolour': 0.3142857142857143,</span></span><br><span class="line"><span class="comment"># 'Virginica': 0.29523809523809524&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-计算类条件概率"><a href="#3-计算类条件概率" class="headerlink" title="3.计算类条件概率"></a>3.计算类条件概率</h4><p>3.1 首先计算每个特征属于每类的条件概率，前面说过这里我们使用概率密度函数来计算</p>
<p>概率密度函数实现：</p>
<p>方差公式：$ var = \frac{\sum(x-avg)^{2}}{n-1}$，概率密度函数：$ p(xi|c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(xi-mean_{c,i})^{2}}{2\sigma_{c,i}^{2}})$ , $\sigma$是标准差（方差开方）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(list)</span>:</span></span><br><span class="line">  list = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> list] <span class="comment">#字符串转数字</span></span><br><span class="line">  <span class="keyword">return</span> sum(list)/float(len(list))</span><br><span class="line"><span class="comment"># 方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">var</span><span class="params">(list)</span>:</span></span><br><span class="line">  list = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> list]</span><br><span class="line">  avg = mean(list)</span><br><span class="line">  var = sum([math.pow((x-avg),<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> list])/float(len(list)<span class="number">-1</span>)</span><br><span class="line">  <span class="keyword">return</span> var</span><br><span class="line"><span class="comment"># 概率密度函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateProb</span><span class="params">(x,mean,var)</span>:</span></span><br><span class="line">    exponent = math.exp(math.pow((x-mean),<span class="number">2</span>)/(<span class="number">-2</span>*var))</span><br><span class="line">    p = (<span class="number">1</span>/math.sqrt(<span class="number">2</span>*math.pi*var))*exponent</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<p>每个属性特征属于每类的条件概率是个组合。举例来说，这里有3个类和4个数值属性，然后我们需要每一个属性（4）和类（3）的组合的类条件概率。</p>
<p>为了得到这12个概率密度函数，那我们需要提前知道这12个属性分别的均值和方差，才可以带入到上述概率密度函数中计算。</p>
<p>计算每个属性的均值和方差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">summarizeAttribute</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset = np.delete(dataset,<span class="number">-1</span>,axis = <span class="number">1</span>) <span class="comment"># delete label</span></span><br><span class="line">    summaries = [(mean(attr),var(attr)) <span class="keyword">for</span> attr <span class="keyword">in</span> zip(*dataset)]</span><br><span class="line">    <span class="keyword">return</span> summaries</span><br><span class="line"></span><br><span class="line">summary = summarizeAttribute(trainset)</span><br><span class="line"><span class="comment">#[(5.758095238095239, 0.7345732600732595),</span></span><br><span class="line"><span class="comment"># (3.065714285714285, 0.18592857142857133),</span></span><br><span class="line"><span class="comment"># (3.5533333333333323, 3.2627051282051274),</span></span><br><span class="line"><span class="comment"># (1.1142857142857148, 0.6014285714285714)]</span></span><br></pre></td></tr></table></figure>
<p>按类别提取属性特征，这里会得到 类别数目*属性数目 组 （均值，方差）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">summarizeByClass</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  dataset_separated,dataset_info = seprateByClass(dataset)</span><br><span class="line">  summarize_by_class = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> classValue, vector <span class="keyword">in</span> dataset_separated.items():</span><br><span class="line">      summarize_by_class[classValue] = summarizeAttribute(vector)</span><br><span class="line">  <span class="keyword">return</span> summarize_by_class</span><br><span class="line"></span><br><span class="line">train_Summary_by_class = summarizeByClass(trainset)</span><br><span class="line"><span class="comment">#&#123;'Setosa': [(4.982926829268291, 0.12445121951219511),</span></span><br><span class="line"><span class="comment">#  (3.3975609756097565, 0.1417439024390244),</span></span><br><span class="line"><span class="comment">#  (1.4707317073170731, 0.03412195121951221),</span></span><br><span class="line"><span class="comment">#  (0.24390243902439032, 0.012024390243902434)],</span></span><br><span class="line"><span class="comment"># 'Versicolour': [(5.933333333333334, 0.2766666666666667),</span></span><br><span class="line"><span class="comment">#  (2.7909090909090906, 0.08960227272727272),</span></span><br><span class="line"><span class="comment">#  (4.254545454545454, 0.23755681818181815),</span></span><br><span class="line"><span class="comment">#  (1.33030303030303, 0.03905303030303031)],</span></span><br><span class="line"><span class="comment"># 'Virginica': [(6.596774193548387, 0.5036559139784946),</span></span><br><span class="line"><span class="comment">#  (2.9193548387096775, 0.10427956989247314),</span></span><br><span class="line"><span class="comment">#  (5.5612903225806445, 0.37711827956989247),</span></span><br><span class="line"><span class="comment">#  (2.0354838709677416, 0.06369892473118278)]&#125;</span></span><br></pre></td></tr></table></figure>
<p>按类别将每个属性的条件概率相乘。</p>
<p>我们前面已经将训练数据集按类别分好，这里就可以实现，输入的测试数据依据每类的每个属性（就那个类别数*属性数的字典）计算属于某类的类条件概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateClassProb</span><span class="params">(input_data,train_Summary_by_class)</span>:</span></span><br><span class="line">  prob = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> class_value, summary <span class="keyword">in</span> train_Summary_by_class.items():</span><br><span class="line">      prob[class_value] = <span class="number">1</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(summary)):</span><br><span class="line">          mean,var = summary[i]</span><br><span class="line">          x = input_data[i]</span><br><span class="line">          p = calculateProb(x,mean,var)</span><br><span class="line">      prob[class_value] *=p</span><br><span class="line">  <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">input_vector = testset[<span class="number">1</span>]</span><br><span class="line">input_data = input_vector[:<span class="number">-1</span>]</span><br><span class="line">train_Summary_by_class = summarizeByClass(trainset)</span><br><span class="line">class_prob = calculateClassProb(input_data,train_Summary_by_class)</span><br><span class="line"></span><br><span class="line"><span class="comment">#&#123;'Setosa': 3.3579279836005993,</span></span><br><span class="line"><span class="comment"># 'Versicolour': 1.5896628317396685e-07,</span></span><br><span class="line"><span class="comment"># 'Virginica': 5.176617264913899e-12&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-先验概率-类条件概率"><a href="#4-先验概率-类条件概率" class="headerlink" title="4.先验概率*类条件概率"></a>4.先验概率*类条件概率</h4><p>朴素贝叶斯分类器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bayesianPredictOneSample</span><span class="params">(input_data)</span>:</span></span><br><span class="line">  prior_prob = calulateClassPriorProb(trainset,train_info)</span><br><span class="line">  train_Summary_by_class = summarizeByClass(trainset)</span><br><span class="line">  classprob_dict = calculateClassProb(input_data,train_Summary_by_class)</span><br><span class="line">  result = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> class_value,class_prob <span class="keyword">in</span> classprob_dict.items():</span><br><span class="line">      p = class_prob*prior_prob[class_value]</span><br><span class="line">      result[class_value] = p</span><br><span class="line">  <span class="keyword">return</span> max(result,key=result.get)</span><br></pre></td></tr></table></figure>
<p>终于把分类器写完啦，接下来就让我们看看测试数据的结果！</p>
<p>单个样本测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input_vector = testset[<span class="number">1</span>]</span><br><span class="line">input_data = input_vector[:<span class="number">-1</span>]</span><br><span class="line">result = bayesianPredictOneSample(input_data)</span><br><span class="line">print(<span class="string">"the sameple is predicted to class: &#123;0&#125;."</span>.format(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the sameple is predicted to class: Versicolour.</span></span><br></pre></td></tr></table></figure>
<p>看看分类准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateAccByBeyesian</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  correct = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> vector <span class="keyword">in</span> dataset:</span><br><span class="line">      input_data = vector[:<span class="number">-1</span>]</span><br><span class="line">      label = vector[<span class="number">-1</span>]</span><br><span class="line">      result = bayesianPredictOneSample(input_data)</span><br><span class="line">      <span class="keyword">if</span> result == label:</span><br><span class="line">          correct+=<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> correct/len(dataset)</span><br><span class="line"></span><br><span class="line">acc = calculateAccByBeyesian(testset)</span><br><span class="line"><span class="comment"># 0.9333333333333333</span></span><br></pre></td></tr></table></figure>
<p>全部代码及数据集已上传至<a href="https://github.com/wenweny/Machine-Learning/tree/master/bayesian">github</a>，第一次写博客不足之处欢迎大家提出交流学习。</p>
<h3 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h3><p><a href="http://python.jobbole.com/81019/" target="_blank" rel="noopener">机器学习之用Python从零实现贝叶斯分类器</a></p>
<p><a href="https://www.zhihu.com/question/21134457/answer/169523403" target="_blank" rel="noopener">知乎-你对贝叶斯统计都有怎样的理解？</a></p>
<p><a href="https://www.cnblogs.com/zhoulujun/p/8893393.html" target="_blank" rel="noopener">贝叶斯公式由浅入深大讲解—AI基础算法入门</a></p>
<p><a href="http://dy.163.com/v2/article/detail/CU0MJOCV05118CTM.html" target="_blank" rel="noopener">先验乘后验贝叶斯定理</a></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>wenwen</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Machine-Learning/"># Machine Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/01/05/%E8%AE%B0%E5%BD%95%E5%86%8D%E4%B8%80%E6%AC%A1%E9%83%A8%E7%BD%B2hexo/">记录再一次利用hexo部署博客</a>
            
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">

	<style>
	.copyright{line-height:24px}
	</style>
	
    <div class="copyright">
        <span>© wenwen | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
		<br>
		<span id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv"></span>  visits  |  </span>	
		
		<span id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv"></span>  visitors</span>
		
	</div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


    </div>
</body>
</html>
